# ğŸ§  AI vs Human Content Detection  

## ğŸ“Œ Overview  
This project aims to distinguish between **AI-generated text** and **Human-written content** using **EDA (Exploratory Data Analysis)** and **Machine Learning models**.  

With the rise of AI tools such as ChatGPT, Bard, and others, detecting AI-generated content has become critical for:  
- ğŸ“š Academic integrity  
- ğŸ“° Journalism & Media authenticity  
- ğŸ” Plagiarism and originality checking  

---

## ğŸ“‚ Project Structure  
```
AI-vs-Human-Content-Detection/
â”‚â”€â”€ README.md
â”‚â”€â”€ AI_vs_Human_Content_Detection_EDA+_Modeling.ipynb   # Full notebook
â”‚â”€â”€ requirements.txt                                    # Dependencies
â”‚â”€â”€ best_model.pkl                                      # Saved ML model
â”‚â”€â”€ main.py                                             # Run predictions from CLI
â”‚â”€â”€ images/                                             # Key visuals
    â”‚â”€â”€ class_distribution.png
    â”‚â”€â”€ readability_scores.png
    â”‚â”€â”€ confusion_matrix.png
```

---

## ğŸ“Š Dataset  
The dataset contains labeled text samples from **AI generators** and **human writers**.  
Each entry includes:  
- **Text content**  
- **Label** (`AI` = 1, `Human` = 0)  

---

## ğŸ” Exploratory Data Analysis (EDA)  

Key insights from the dataset:  

### 1ï¸âƒ£ Class Distribution  
Balanced dataset with both AI and Human samples.  

![Class Distribution](images/class_distribution.png)  

### 2ï¸âƒ£ Readability Scores  
AI text often shows lower variance in readability scores compared to human text.  

![Readability Scores](images/readability_scores.png)  

### 3ï¸âƒ£ Confusion Matrix (Final Model)  
Our final trained model achieved strong classification accuracy.  

![Confusion Matrix](images/confusion_matrix.png)  

---

## ğŸ¤– Modeling  

We trained and evaluated multiple ML models:  
- Logistic Regression  
- Random Forest  
- XGBoost  
- LightGBM  

After tuning, **XGBoost** gave the best balance of precision and recall.  
The trained model is saved as **`best_model.pkl`**.  

---

## âš™ï¸ Installation  

Clone the repository and install dependencies:  

```bash
git clone https://github.com/YOUR_USERNAME/AI-vs-Human-Content-Detection.git
cd AI-vs-Human-Content-Detection
pip install -r requirements.txt
```

---

## ğŸš€ Usage  

You can run predictions directly from the command line using `main.py`:  

```bash
python main.py "This is a sample text to check if AI wrote it."
```

---

## ğŸ“Œ Example Run  

```bash
python main.py "This text is generated by an AI."
```
**Output:**  
```
Predicted: AI
```

```bash
python main.py "I love writing stories late at night with a cup of tea."
```
**Output:**  
```
Predicted: Human
```

---

## ğŸ“ˆ Results  

- **Accuracy:** ~XX% (update with your final score)  
- **Best Model:** XGBoost  
- **Key Finding:** AI-generated text tends to have lower lexical diversity and more uniform structure compared to human text.  

---

## ğŸ›  Tech Stack  
- Python 3.x  
- NumPy, Pandas  
- Matplotlib, Seaborn  
- Scikit-learn  
- XGBoost, LightGBM  
- Joblib  

---

## ğŸ“œ License  
This project is open-source and available under the **MIT License**.  
